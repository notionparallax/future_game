live: true
body:
    paragraphs:
        - Bias can be built into algorithms in several ways. The obvious one is through the datasets used to train the system. If a dataset has only cats, it will classify a dog as a cat. It can also be introduced through the hyperparameters (the setup variables of the system).
        - Propublica[^propublica_bias] showed widespread bias in sentencing based on an algorithm that baked in, amongst other things, geographical biases, e.g. if you live in a black neighbourhood, sentence more harshly.
        - Many ML algorithms are considered "black boxes" so can't be explained. There is a lot of work on trying to make that not the case.
card_type: technology
consider:
    - Whose responsibility is it to make sure that algorithms don't discriminate against particular groups of people?
    - Is there an acceptable rate of failure? What is that relative to the human rate of failure?
footnotes:
    "propublica_bias": "[Machine Bias - There’s software used across the country to predict future criminals. And it’s biased against blacks. by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, ProPublica May 23, 2016](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)"
image:
    caption: The PULSE algorithm takes pixelated faces and turns them into high-resolution images. However, it has a strong bias towards white faces. ∴ Obama -> white guy.
    citation: The Verge
    link: https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias
    source: face_depixelizer_obama.jpg
title: AI Bias
